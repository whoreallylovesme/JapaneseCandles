{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKDUCkXjpdwjOwgLk0gGQl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoreallylovesme/JapaneseCandles/blob/main/candles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njzi2LGV6tzP"
      },
      "outputs": [],
      "source": [
        "# importing some PySpark libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# importing some python3 libraries\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# без этой строчки про парсер ничего работать не будет\n",
        "# (как я понял это проблемы современного спарка)\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"SparkTask\") \\\n",
        "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "input_csv = \"fin_sample.csv\"\n",
        "path = input(\"Enter PATH to CSV file or just press ENTER for std df: \")\n",
        "if (path):\n",
        "    input_csv = path"
      ],
      "metadata": {
        "id": "vhfp8j-bCZRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading df\n",
        "schema = StructType([\n",
        "    StructField(\"#SYMBOL\", StringType(), True),\n",
        "    StructField(\"SYSTEM\", StringType(), True),\n",
        "    StructField(\"MOMENT\", StringType(), True),\n",
        "    StructField(\"ID_DEAL\", LongType(), True),\n",
        "    StructField(\"PRICE_DEAL\", DoubleType(), True),\n",
        "    StructField(\"VOLUME\", LongType(), True),\n",
        "    StructField(\"OPEN_POS\", LongType(), True),\n",
        "    StructField(\"DIRECTION\", StringType(), True),\n",
        "])\n",
        "df = spark.read.csv(input_csv, header=True, schema=schema)\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "mpLn1JogDFAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just setting std options\n",
        "options = {}\n",
        "options[\"candle.width\"] = 300000\n",
        "options[\"candle.date.from\"] = 19000101\n",
        "options[\"candle.date.to\"] = 20200101\n",
        "options[\"candle.time.from\"] = 1000\n",
        "options[\"candle.time.to\"] = 1800\n",
        "\n",
        "# parsing xml config file from command line argument\n",
        "xml_file_path = input(\n",
        "    \"Enter PATH to XML config file or just press ENTER for std options: \"\n",
        ")\n",
        "if (xml_file_path):\n",
        "    tree = xml.etree.ElementTree.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "    for prop in root.findall('property'):\n",
        "        name = prop.find('name').text\n",
        "        value = prop.find('value').text\n",
        "        options[name] = value"
      ],
      "metadata": {
        "id": "RbT4mMa7DS_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to TIMESTAMP format\n",
        "df = df.withColumn(\"MOMENT\", F.to_timestamp(\"MOMENT\", \"yyyyMMddHHmmssSSS\"))\n",
        "\n",
        "# filtering according options\n",
        "df = df.filter(\n",
        "    (F.hour(\"MOMENT\") * 100 + F.minute(\"MOMENT\") >= options[\"candle.time.from\"]) &\n",
        "    (F.hour(\"MOMENT\") * 100 + F.minute(\"MOMENT\") <= options[\"candle.time.to\"])\n",
        ")\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "Q-rmIA_vNEZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making candlesticks\n",
        "\n",
        "# splitting by time windows\n",
        "window_spec = F.window(\"MOMENT\", f\"{options['candle.width']} milliseconds\")\n",
        "\n",
        "# group by #SYMBOL and time windows\n",
        "# then aggregate it for answer according aliases\n",
        "candlestick_df = df.groupBy(\"#SYMBOL\", window_spec).agg(\n",
        "    F.first(\"PRICE_DEAL\").alias(\"OPEN\"),\n",
        "    F.max(\"PRICE_DEAL\").alias(\"HIGH\"),\n",
        "    F.min(\"PRICE_DEAL\").alias(\"LOW\"),\n",
        "    F.last(\"PRICE_DEAL\").alias(\"CLOSE\"),\n",
        "    F.sum(\"VOLUME\").alias(\"Volume\")\n",
        ")\n",
        "\n",
        "# select required columns for answer\n",
        "candlestick_df = candlestick_df.select(\n",
        "    F.col(\"#SYMBOL\"),\n",
        "    F.col(\"window.start\").alias(\"MOMENT\"),\n",
        "    \"OPEN\",\n",
        "    \"HIGH\",\n",
        "    \"LOW\",\n",
        "    \"CLOSE\"\n",
        ")\n",
        "\n",
        "# order by time (ascending)\n",
        "candlestick_df = candlestick_df.orderBy(\"#SYMBOL\", \"MOMENT\")\n",
        "\n",
        "# converting back to yyyyMMddHHmmssSSS from TIMESTAMP\n",
        "candlestick_df = candlestick_df.withColumn(\n",
        "    \"MOMENT\",\n",
        "    F.date_format(\"MOMENT\", \"yyyyMMddHHmmssSSS\")\n",
        ")\n",
        "\n",
        "candlestick_df.show(5)"
      ],
      "metadata": {
        "id": "l16QydmtTELN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing answer\n",
        "\n",
        "# selecting unique #SYMBOL\n",
        "unique_symbols = candlestick_df \\\n",
        "                 .select(\"#SYMBOL\") \\\n",
        "                 .distinct() \\\n",
        "                 .rdd \\\n",
        "                 .flatMap(lambda x: x) \\\n",
        "                 .collect()\n",
        "\n",
        "output_dir = \"AnswerSparkTask\"\n",
        "\n",
        "# rm -r output_dir\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir)\n",
        "\n",
        "# writing answer to files\n",
        "for symbol in unique_symbols:\n",
        "    symbol_df = candlestick_df.filter(F.col(\"#SYMBOL\") == symbol)\n",
        "    output_path = os.path.join(output_dir, f\"{symbol}.csv\")\n",
        "    symbol_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "    # print some logs\n",
        "    print(f\"Saved candlestick data for {symbol} to {output_path}\")"
      ],
      "metadata": {
        "id": "TgxIrRC8lIYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}